\chapter{Row Reduction Method }
In this chapter, our primary focus revolves around the application of the row reduction method to both finite and infinite matrices. In the finite case, we employ this method to solve systems of linear equations effectively. Conversely, for infinite matrices, we present a theorem delineating a systematic approach to solving infinite systems of linear equations. This theorem is rooted in the foundational principles of the corresponding vector space and the rank of the coefficient matrix.
\section{Row Reduction Method of Finite Matrices}
\begin{definition}
Let $A$ be a $m \times n$ matrix. We say that $A$ is in row echelon form if and only if all its non-zero rows contain a pivot and all its zero rows are located below the non-zero rows.\newline
In the definition above, a zero row is a row whose entries are all equal to zero, and a non-zero row is a row that has at least one element different from zero.
    
\end{definition}

\begin{example}
\[
 A = 
    \begin{pmatrix}
    \underbar2 & 5 & 7\\
    0 & \underbar6 & 1\\
    0 & 0 & \underbar1

\end{pmatrix}
\]
   
The matrix $A$ is in row echelon form.The first, second and third  row have  pivot $A_{11}$, $A_{22}$, $A_{33}$ respectively i.e. the first nonzero entry of each row.
\end{example}

\paragraph{Application of row reduction method :}
Let us solve the system of three equations in three unknowns, and the system of linear equations in the following.
\[
\begin{rcases}
    2x + y - z &= 8 \\
-3x - y + 2z &= -11 \\
-2x + y + 2z &= -3 \\
\end{rcases}
\]
or equivalently, in matrix form:
\[
\begin{bmatrix}
2 & 1 & -1 & | & 8 \\
-3 & -1 & 2 & | & -11 \\
-2 & 1 & 2 & | & -3 \\
\end{bmatrix}
\]

The first pivot is chosen as the element in the $(1,1)$ position, which is $2$, and dividing the first row by $2$ to make the pivot element at $(1, 1)$ equal to $1$.
Performing row operations as follows:
\begin{center}
    \begin{align*}
        R_2^{'} = R_2 + 3R_1\\
        R_1^{'} = R_1 + 2R_3
    \end{align*}
\end{center}
We obtained the matrix as follows:
\[
\begin{bmatrix}
1 & \frac{1}{2} & -\frac{1}{2} & | & 4 \\
0 & \frac{1}{2} & \frac{1}{2} & | & 1 \\
0 & 2 & 1 & | & 5 \\
\end{bmatrix}
\] 
Now multiply the second row by $2$ to make the pivot element at $(2, 2)$ equal to $1$ and perform row operations as follows:
\begin{center}
    \begin{align*}
        R_2^{'} = R_1 +(\frac{-1}{2})R_2 \\
        R_3^{'} = R_3 +(-2)R_2
    \end{align*}
\end{center}
We get a matrix as follows :
\[
% \xRightarrow[] {}
\begin{bmatrix}
1 & 0 & -1 & | & 3 \\
0 & 1 &  1 & | & 2 \\
0 & 0 & -1 & | & 1 \\
\end{bmatrix}
\]
By using further row operation we find the matrix as follows:
\begin{center}
    \begin{align*}
        R_1^{'} = R_1 + (-2)R_3 \\
        R_2^{'} = R_2 + 3R_3
    \end{align*}
\end{center}
\[
% \xRightarrow[]{}
\begin{bmatrix}
1 & 0 & 2 & | & 2 \\
0 & 1 & -3 & | & 3 \\
0 & 0 & 7 & | & -1 \\
\end{bmatrix}
\]
Repeat the process to choose a new pivot (in this case, element $(2,2)$) and make it $1$, then eliminate other elements in the pivot column and perform row operations as follows:
\begin{center}
    \begin{align*}
        R_1^{'} = R_1 + R_3 \\
        R_2^{'} = R_2 + (-1) R_3
    \end{align*}
\end{center}
\[
% \xRightarrow[]{}
\begin{bmatrix}
1 & 0 & 0 & | & 2 \\
0 & 1 & 0 & | & 3 \\
0 & 0 & 1 & | & -1 \\
\end{bmatrix}
\]


which is in row echelon form. So we can use the back substitution algorithm. All columns contain pivot so that there are only basic variables. We start from the last row and solve for the basic variable corresponding to its pivot\newline
\begin{align*}
    z = -1
\end{align*}

        
Then we move to the second row and again we solve for the basic variable corresponding to its pivot
\begin{align*}
    y = 3
\end{align*}
and again for the first row
\begin{align*}
        x = 2
\end{align*}

\text{So, the solution to the system of linear equations is}

\[ x = 2, \quad y = 3, \quad z = -1. \]




\section{Row Reduction Method of Infinite Matrices}
\begin{theorem}
    Let $m, n \in \mathbb{N} \cup \{\infty\}$. A system of $m$ linear equations in $n$ variables $Ax = b$ is compatible if and only if both the consistent and complete matrices ($A$ and $[A|b]$ respectively) are characterized by the same rank, i.e., $\text{rank}(A) = \text{rank}([A|b])$.\cite{amgopaper
\medskip
\newline
\textbf{Proof:} Let $m, n \in \mathbb{N} \cup \{\infty\}$. The system of linear equations $Ax = b$ can be interpreted as a linear mapping $f : \mathbb{F}^n \rightarrow \mathbb{F}^m$, where $f(x) = Ax$, and $A \in M(n, R)$, where $R$ is an integral domain.

The system is determined if there exists at least one solution, i.e., if there exists $x_0$ such that $f(x_0) = b$. This implies that the system is determined if $b \in \text{Im}(f)$.

The basis spanning the image vector space ($\text{Im}(f)$, $+$, $\cdot$) is composed of the column vectors of the matrix $A$
\[ B_{\text{Im}(f)} = \{I_1, I_2, \ldots, I_n\}, \quad A \in \{I_1, I_2, \ldots, I_n\}. \]

Thus, the system is consistent if and only if $b$ belongs to the span of the column vectors of the matrix $A$.
\[ b \in \{I_1, I_2, I_3, \ldots\}. \] \newline
This is equivalent to saying that the rank of \[A = (I_1 \  I_2 \  I_3 \ldots)\] and \[[A|b] = (I_1 \  I_2 \  I_3 \ldots |b)\]have the same rank. Therefore, the system is consistent if $\operatorname{rank}(A) = \operatorname{rank}([A|b])$
\end{theorem}

.\newline




